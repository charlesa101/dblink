// Copyright (C) 2018  Australian Bureau of Statistics
//
// Author: Neil Marchant
//
// This file is part of dblink.
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

import com.github.ngmarchant.dblink.SimilarityFn._
import com.github.ngmarchant.dblink._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, SparkSession}

object NLTCS extends Logging {

  final val local = true // whether running on laptop (true) or MUPPET (false)
  final val full = true // whether to use the full dataset (true) or a single block (false)

  // Directory used for saving Spark checkpoints
  val checkpointPath: String =
    if (local) "/home/nmarchant/SparkCheckpoint/"
    else "/scratch/neil/SparkCheckpoint/"

  // File URI for the source data files
  val filePath: String =
    if (local) "/home/nmarchant/Dropbox/Employment/AMSIIntern ABS/experiments/nltcs-data/proc_nltcs_*.txt"
    else "/data/neil/proc_nltcs_*.txt"

  // Directory used for saving samples + state
  val projectPath: String =
    if (local) "/home/nmarchant/nltcs_0levels_med-prior_collapsed/"
    else "/data/neil/nltcs_5levels_med-prior_collapsed/"

  // Schema for the source data files (used to read as DataFrame)
  val recordsSchema = StructType(
    StructField("SEQ", StringType, nullable=false) ::
      StructField("SEX", StringType, nullable=false) ::
      StructField("DOB_DAY", StringType, nullable=false) ::
      StructField("DOB_MONTH", StringType, nullable=false) ::
      StructField("DOB_YEAR", StringType, nullable=false) ::
      StructField("STATE", StringType, nullable=false) ::
      StructField("REGOFF", StringType, nullable=false) :: Nil
  )

  // Maps a file path to a unique file identifier
  val pathToFileId: String => String = (path: String) => {
    path.split("/").last.substring(11, 13)
  }

  // Column names
  val attributeSpecs: Array[AbstractAttribute] = Array(
    Attribute("SEX", ConstantSimilarityFn),
    Attribute("DOB_DAY", ConstantSimilarityFn),
    Attribute("DOB_MONTH", ConstantSimilarityFn),
    Attribute("DOB_YEAR", ConstantSimilarityFn),
    Attribute("STATE", ConstantSimilarityFn),
    Attribute("REGOFF", ConstantSimilarityFn)
  )
  val recIdColName: String = "rec_id"
  val fileIdColName: Option[String] = Some("file_id") // not needed for one file
  val entIdColName: Option[String] = Some("ent_id")

  // Partitioning
  val partitionLevels: Int = 0
  val partitionVars: Seq[Int] = Seq(2, 1, 3, 4)

  /** Loads the source data files into a Spark DataFrame using the
    * recordsSchema. No transformations are applied.
    */
  lazy val dataFrame: DataFrame = {
    val spark = SparkSession.builder().getOrCreate()
    import spark.implicits._
    spark.udf.register("get_file_name", pathToFileId)

    val df = if (full) {
      spark.read.format("csv")
        .option("header", "false")
        .option("delimiter", " ")
        .option("mode", "DROPMALFORMED")
        .schema(recordsSchema)
        .load(filePath)
    } else {
      spark.read.format("csv")
        .option("header", "false")
        .option("delimiter", " ")
        .option("mode", "DROPMALFORMED")
        .schema(recordsSchema)
        .load(filePath)
        .filter(_.getAs[String]("DOB_MONTH") == "12")
    }

    df.withColumnRenamed("SEQ", "ent_id")
      .withColumn("file_id", callUDF("get_file_name", input_file_name))
      .withColumn("rec_id", concat($"file_id", $"ent_id"))
  }

  lazy val numRecords: Long = dataFrame.count()

  val alpha: Double = numRecords * 0.1 * 0.01
  val beta: Double = numRecords * 0.1

  lazy val parameters = Parameters(
    distortionPrior = Array.fill(attributeSpecs.length)(BetaShapeParameters(alpha, beta)),
    numEntities = numRecords,
    randomSeed = 319158L,
    maxClusterSize = 2 + 8
  )

  def apply(): EBER = new EBER(projectPath, checkpointPath, dataFrame, recIdColName, fileIdColName,
    attributeSpecs, parameters, partitionLevels, partitionVars, entIdColName)
}